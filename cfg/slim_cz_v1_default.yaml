# ============================================================================
# SLiM-CZ-V1 Default Configuration
# Slavic Linguistic integrated Micro-model for Czechia
# ============================================================================
#
# Optimized for small Czech datasets (2-5M tokens)
# Based on research: Chinchilla scaling laws recommend 100-150k params per 1M tokens
# This config uses ~3-5M params with aggressive regularization for better quality
#
# Research optimizations applied:
#   - Weight tying (saves ~30% parameters)
#   - Enhanced dropout (0.25 vs standard 0.1)
#   - Aggressive weight decay (0.05 vs 0.01)
#   - Label smoothing (0.1 prevents overconfidence)
#   - Extended training (30 epochs vs 10)
#   - SentencePiece BPE tokenizer optimized for Czech morphology
# ============================================================================

# Model architecture
model:
  vocab_size: 16000
  d_model: 256
  num_heads: 8
  num_layers: 4
  d_ff: 1024
  max_seq_len: 512
  dropout: 0.25
  weight_tying: true

# Training parameters
train:
  # Optimization
  batch_size: 32
  learning_rate: 0.0001
  weight_decay: 0.05
  betas: [0.9, 0.98]
  eps: 1.0e-9
  
  # Learning rate schedule
  warmup_steps: 500
  scheduler: cosine
  min_lr: 1.0e-6
  
  # Regularization
  gradient_clip: 1.0
  label_smoothing: 0.1
  
  # Training duration
  epochs: 30
  max_steps: 100000
  
  # Logging and evaluation
  log_every: 50
  eval_every: 500
  save_every: 2000
  
  # Early stopping
  patience: 8
  min_delta: 0.0005
  
  # Optional monitoring
  use_tensorboard: false

# Generation parameters
generation:
  max_new_tokens: 100
  temperature: 0.8
  top_k: 50
  top_p: 0.95
  repetition_penalty: 1.2
  do_sample: true

# Data preprocessing
data:
  # Dataset splits
  train_split: 0.90
  val_split: 0.05
  test_split: 0.05
  
  # Sequence configuration
  seq_len: 512
  stride: 256
  
  # Text processing
  lowercase: false
  remove_urls: true
  remove_emails: true
  min_line_length: 10
  min_frequency: 2
