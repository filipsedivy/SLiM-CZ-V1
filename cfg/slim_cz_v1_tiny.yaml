# ============================================================================
# SLiM-CZ-V1 Tiny Configuration
# ============================================================================
#
# Optimized for:
#   - Very small datasets (< 2M tokens)
#   - Fast experimentation
#   - Resource-constrained environments
#
# Model size: ~1-2M parameters
# ============================================================================

# Model architecture
model:
  vocab_size: 12000
  d_model: 192
  num_heads: 6
  num_layers: 3
  d_ff: 768
  max_seq_len: 256
  dropout: 0.3
  weight_tying: true

# Training parameters
train:
  # Optimization
  batch_size: 64
  learning_rate: 0.0003
  weight_decay: 0.1
  betas: [0.9, 0.98]
  eps: 1.0e-9
  
  # Learning rate schedule
  warmup_steps: 300
  scheduler: cosine
  min_lr: 1.0e-6
  
  # Regularization
  gradient_clip: 1.0
  label_smoothing: 0.15
  
  # Training duration
  epochs: 40
  max_steps: 50000
  
  # Logging and evaluation
  log_every: 25
  eval_every: 250
  save_every: 1000
  
  # Early stopping
  patience: 10
  min_delta: 0.001
  
  # Optional monitoring
  use_tensorboard: false

# Generation parameters
generation:
  max_new_tokens: 50
  temperature: 0.9
  top_k: 40
  top_p: 0.95
  repetition_penalty: 1.3
  do_sample: true

# Data preprocessing
data:
  # Dataset splits
  train_split: 0.90
  val_split: 0.05
  test_split: 0.05
  
  # Sequence configuration
  seq_len: 256
  stride: 128
  
  # Text processing
  lowercase: false
  remove_urls: true
  remove_emails: true
  min_line_length: 10
  min_frequency: 2
